"""
LLM Provider Abstraction Layer
Supports: local_llm (OpenAI-compatible), openai, mock
"""
import os
import logging
import httpx
from typing import Optional, List, Dict, Any
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    async def generate(self, prompt: str, system_message: str = "", max_tokens: int = 4000, temperature: float = 0.7) -> str:
        pass
    
    @abstractmethod
    async def is_available(self) -> bool:
        pass

class MockProvider(LLMProvider):
    """Mock provider for testing without API calls"""
    
    async def generate(self, prompt: str, system_message: str = "", max_tokens: int = 4000, temperature: float = 0.7) -> str:
        return f"""# Mock Generated Content

This is a mock response for testing purposes.

## Prompt Received
{prompt[:500]}...

## System Message
{system_message[:200] if system_message else 'None'}

---
*Generated by MockProvider - No LLM API was called*
"""
    
    async def is_available(self) -> bool:
        return True

class OpenAIProvider(LLMProvider):
    """OpenAI API provider"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model
        self._client = None
    
    @property
    def client(self):
        if self._client is None:
            from openai import AsyncOpenAI
            self._client = AsyncOpenAI(api_key=self.api_key)
        return self._client
    
    async def generate(self, prompt: str, system_message: str = "", max_tokens: int = 4000, temperature: float = 0.7) -> str:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response.choices[0].message.content
    
    async def is_available(self) -> bool:
        return bool(self.api_key)

class LocalLLMProvider(LLMProvider):
    """Local LLM provider (OpenAI-compatible API)"""
    
    def __init__(self, base_url: str, api_key: str = "not-needed", model: str = "local-model"):
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.model = model
    
    async def generate(self, prompt: str, system_message: str = "", max_tokens: int = 4000, temperature: float = 0.7) -> str:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{self.base_url}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            )
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]
    
    async def is_available(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/v1/models")
                return response.status_code == 200
        except Exception:
            return False

class LLMService:
    """Main LLM Service - handles provider selection and fallback"""
    
    def __init__(self):
        self.provider: Optional[LLMProvider] = None
        self.provider_name: str = "none"
        self._initialize()
    
    def _initialize(self):
        provider_type = os.environ.get("LLM_PROVIDER", "mock").lower()
        
        if provider_type == "local_llm":
            base_url = os.environ.get("LOCAL_LLM_BASE_URL", "http://localhost:5002")
            api_key = os.environ.get("LOCAL_LLM_API_KEY", "not-needed")
            model = os.environ.get("LOCAL_LLM_MODEL", "local-model")
            self.provider = LocalLLMProvider(base_url, api_key, model)
            self.provider_name = "local_llm"
            logger.info(f"LLM Provider: local_llm @ {base_url}")
            
        elif provider_type == "openai":
            api_key = os.environ.get("OPENAI_API_KEY")
            model = os.environ.get("OPENAI_MODEL", "gpt-4o")
            if api_key:
                self.provider = OpenAIProvider(api_key, model)
                self.provider_name = "openai"
                logger.info(f"LLM Provider: openai ({model})")
            else:
                logger.warning("OpenAI provider selected but OPENAI_API_KEY not set, falling back to mock")
                self.provider = MockProvider()
                self.provider_name = "mock"
                
        else:  # mock or default
            self.provider = MockProvider()
            self.provider_name = "mock"
            logger.info("LLM Provider: mock (no API calls)")
    
    async def generate(self, prompt: str, system_message: str = "", max_tokens: int = 4000, temperature: float = 0.7) -> str:
        """Generate content using the configured provider"""
        if not self.provider:
            raise RuntimeError("No LLM provider configured")
        
        try:
            return await self.provider.generate(prompt, system_message, max_tokens, temperature)
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            raise
    
    async def is_available(self) -> bool:
        """Check if the provider is available"""
        if not self.provider:
            return False
        return await self.provider.is_available()
    
    def get_status(self) -> Dict[str, Any]:
        """Get provider status info"""
        return {
            "provider": self.provider_name,
            "configured": self.provider is not None
        }

# Global instance
llm_service = LLMService()
